{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1df981aa6f8b9c21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Laserscanning - Exercise 9\n",
    "PyTorch and deep learning.\n",
    "Please, use this notebook locally on your own machine as the cluster is currently in a fragile state. If you are facing any problem you can not solve, feel free to contact me via email!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8eae80fa44f2d7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Please upload the implemented solutions until <u>31.01.2023</u> to the studip folder of your group. The file should follow this format:\n",
    "##### EX09_Group_XX.ipynb (e.g. EX09_Group_04.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please also edit the following:\n",
    "\n",
    "<u>Group XX:</u>\n",
    "\n",
    "| Firstname | Lastname |\n",
    "| :--- | :--- |\n",
    "| firstname1 | lastname3 |\n",
    "| firstname2 | lastname3 |\n",
    "| firstname3 | lastname3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-939f95f48b86636e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b02c6f8ab3b7a67d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Getting to know PyTorch\n",
    "We will use a library called PyTorch for our experiments. It integrates well with Python and the `numpy` library. It is [very well documented](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "It has tensor manipulation capabilities which are often analogous to numpy. However, it goes beyond that in that it allows to put computations either on the CPU or on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.0, 2.0, 1.0] # List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a)  # Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(a) # PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a).dtype, torch.tensor(a).dtype  # np has float64 as default, torch has float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(24); t  # arange as in np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.view(3,8)  # view generates a view with different dimensions, of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.view(2,3,4)  # Same data as 2x3x4 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.view(2,3,-1)  # Can also determine remaining dimension by itself (note the -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.view(2,-1)  # Another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = t.view(4,6); v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.t()  # Transpose. Only for 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = t.view(2,3,4); v  # May think of: channels x rows x colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.transpose(0,1)  # Transpose dimensions 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.transpose(1,2)  # Transpose dimensions 1 and 2. Think of: channels x columns x rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v  # Show again. Think channels x rows x columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.permute(1,2,0)  # Think channels x rows x colums --> rows x columns x channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.permute(1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3])  # When data is integer, torch uses int64.\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1.0,2,3])  # When data is float, torch uses float32.\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3], dtype=torch.float64)  # To enforce a certain datatype, use dtype=...\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3])  # t is int64.\n",
    "print(t.dtype)\n",
    "u = t.double()  # Convert to float64.\n",
    "print(u.dtype)\n",
    "v = t.to(torch.double)  # Also convert to float64.\n",
    "print(v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c97042db3a8f0806",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## That's what it's all good for: put the data on the GPU\n",
    "...if you have one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = t.to(device=\"cuda\")  # Uncomment this if you have a GPU. Otherwise, it will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52f497299aff3ad6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Autograd: computing the gradient automatically\n",
    "As we have learned, one essential part of optimization is computing the gradient. 'Layered' structures lead to nested function calls, e.g. $x\\rightarrow y=f_1(x)\\rightarrow z=f_2(y)$ leads to $f_2(f_1(x))$. Then, for the derivative, we need to apply the chain rule, which eventually leads to the backpropagation algorithm shown in the lecture.\n",
    "\n",
    "## Computing the function value\n",
    "\n",
    "Let's do an example, assuming three layers, denoted as $l$, $m$ and $n$, computing the function $n(m(l(x,y))$. During a *forward pass*, we would be interested in the function value $v = n(m(l(x_0,y_0))$, for given input values $(x_0,y_0)^\\top$. For example, the $v$ would be the value of the loss for that particular input.\n",
    "\n",
    "Then, for optimization, we would need the derivative of $v$ with respect to $x$ and $y$, taken at the particular input $x_0$, $y_0$. This can be computed using a *backward pass*, called *backpropagation*.\n",
    "\n",
    "So the sequence is:\n",
    "- We start from the 'input layer' $l$, having two variables, $l=(x,y)^\\top$.\n",
    "- Then, our second layer $m$ computes two new values, $m=(l_1^2, \\sin(l_2))^\\top$.\n",
    "- Finally, our third layer $n$ computes the square root of the sum: $n = \\sqrt{m_1 + m_2}$.\n",
    "\n",
    "Let's do the computation for the particular values $(x_0, y_0) = (1,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 1.0, 2.0\n",
    "l = torch.tensor([x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3db35cc767e9acfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We take the square of the first element and the sine of the second element and combine it back to a two element tensor using `stack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.stack((l[0]*l[0], torch.sin(l[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2152d30e861e67b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, we compute the sum of both elements and take the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.sqrt(m.sum())\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed02514ff7a7ce02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That is, for the input $(x_0, y_0) = (1,2)$, the function value of $v = \\sqrt{x_0^2 + \\sin(y_0)}$ is $1.3818$.\n",
    "\n",
    "## Computing the gradient\n",
    "Now to compute the gradient with respect to x and y, we would usually do the symbolic computation:\n",
    "- $\\frac{\\partial}{\\partial x}\\sqrt{x^2 + \\sin(y)} = \\frac{1}{2}\\left( x^2 + \\sin(y)\\right){}^{-1/2} \\cdot 2 \\cdot x$, and\n",
    "- $\\frac{\\partial}{\\partial y}\\sqrt{x^2 + \\sin(y)} = \\frac{1}{2}\\left( x^2 + \\sin(y)\\right){}^{-1/2} \\cdot \\cos(y)$.\n",
    "\n",
    "Then, we would evaluate those two values at $(x_0, y_0)$. Let's do this 'by hand':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "ddx = 0.5 * (x*x + sin(y))**(-0.5) * 2 * x\n",
    "ddy = 0.5 * (x*x + sin(y))**(-0.5) * cos(y)\n",
    "torch.tensor((ddx, ddy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5aacf8d012bb1d98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Automatic computation of the gradient: autograd\n",
    "This worked, but was quite tedious. We had do to a symbolic computation, and the formulas we got are quite long. From the lecture, we know they will get longer with each layer, and to evaluate them, we will re-compute subparts of the formulas multiple times. This re-computation overhead is solved by backpropagation, which tabulates values and re-uses them.\n",
    "\n",
    "As it turns out, PyTorch has backpropagation built in.\n",
    "\n",
    "To start, we do the exactly same thing as above for the forward pass, but **switch on gradient computation**. This is important, as it will instruct PyTorch to keep track of all computations we do with the tensor. While we pile up algebraic manipulations, PyTorch will secretly build a graph of operations, which it will use later to perform backpropagation.\n",
    "\n",
    "So the following computation is exactly the same as above, with the exception that we switch on the gradient for the initial tensor $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 1.0, 2.0\n",
    "l = torch.tensor([x, y], requires_grad=True)  # <-- Note the 'requires_grad=True'.\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.stack((l[0]*l[0], torch.sin(l[1])))\n",
    "n = torch.sqrt(m.sum())\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-331a088b10e8b1b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So the function value is the same as above. To get the gradient, we just call the backpropagation on $n$, then grab the gradient from $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.backward()\n",
    "l.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f17f78a8878cea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That is the same as our result from above, which we obtained based on the hand-computed symbolic derivative. So amazingly, **without doing any symbolic computation, we get the gradient**. There is three different ways to compute derivatives: symbolically (what we did above by hand), numerically (working with the difference quotient, $(f(x+\\varepsilon)-f(x))/\\varepsilon$), and automatically see [here](https://en.wikipedia.org/wiki/Automatic_differentiation) if you are interested. PyTorch is using the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91277a9bd7189dde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Autograd accumulates gradients\n",
    "Note that autograd **accumulates the gradients**. This is by design, because the normal application is to sum up the loss function, computed for every sample (in a batch or minibatch). Thus, the gradient is the sum of gradients.\n",
    "\n",
    "See what happens if we compute the function $n$ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.stack((l[0]*l[0], torch.sin(l[1])))\n",
    "n = torch.sqrt(m.sum())\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-907e54f411b0531e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Of course, we get the same function value. What happens to our gradients (stored in $l$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.backward()\n",
    "l.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba0b649f01cd6b98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note this is twice the values from above. PyTorch has accumulated the gradient in $l$. If this is not intended, you must remove or zero the gradient. E.g. in a training loop, one will zero the gradient after it has been used in an update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.grad.zero_()  # <-- Zero the gradient. Alternatively one could use: l.grad = None\n",
    "m = torch.stack((l[0]*l[0], torch.sin(l[1])))\n",
    "n = torch.sqrt(m.sum())\n",
    "n.backward()\n",
    "l.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70f293fbe135c988",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Plane estimation using PyTorch\n",
    "\n",
    "In the lecture, we learned that a single layer perceptron with a MSE loss does in fact a least squares estimation of a plane.\n",
    "\n",
    "We will first do a standard least squares estimation of a plane.\n",
    "\n",
    "## Standard least squares estimation\n",
    "First, we define some random $(x_1, x_2)$ samples (uniformly distributed in $[-1,1]$). We print out the first five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000\n",
    "X = torch.rand((M,2)) * 2 - 1\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1631dc7750d52551",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the following purposes, it will be useful to have a third column which is all ones. So we re-cast our `X`, the first two columns will be the old `X` and the last column will be all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.empty((M,3))\n",
    "tmp[:,0:2] = X\n",
    "tmp[:,2] = 1\n",
    "X = tmp\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e4f41604ee54e8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Remember the plane equation is $y = w_1 x_1 + w_2 x_2 + b$. Using the matrix $X$, we can now simply obtain the $y$ vector by matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_known, w2_known, b_known = 2.0, 4.0, 1.0\n",
    "y_exact = X @ torch.tensor((w1_known, w2_known, b_known))\n",
    "y_exact[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6da47fba028da3f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Normally, we do not know `w1_known`, `w2_known`, and `b_known`. We only get random samples that were generated by a process, which produces points on a plane with added random noise. In our case, it is Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.normal(mean=y_exact, std=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06dd7b43f3497a53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us plot these points: they are on a plane, with added noise, as we intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "ipv.clear()\n",
    "ipv.scatter(X[:,0].numpy(), X[:,1].numpy(), y.numpy(), marker=\"sphere\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efaecf4286b4f4f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we pretend that we do know the model that produced these random points, but we do not know the parameters. To estimate them, we can do the standard least squares estimation, $\\hat{\\theta} = (X^\\top X)^{-1}X^\\top y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X.t() @ X).inverse() @ X.t() @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa6b18c745705518",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That worked well! The result is close to the `w1_known`, `w2_known`, and `b_known` parameters which we used above to generate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb387c69a0ce4ea4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Using optimization to fit parameters\n",
    "Now we want to proceed to a general optimization approach. As we know, in our linear estimation problem above, this actually makes no sense, because we can figure out the least squares solution in closed form. That is, there is no iteration necessary. However, as soon as we add some nonlinearities, this would not work anymore.\n",
    "\n",
    "First of all, we define our model. It is $y = w_1 x_1 + w_2 x_2 + b$, so it is just the scalar product of $(x_1, x_2, 1)$ and $\\theta=(w_1, w_2, b)$, which can be conveniently expressed as `x @ theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, theta):\n",
    "    return x @ theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e523f2d036ab72e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we define our loss function, the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y, y_ref):\n",
    "    diff = y - y_ref\n",
    "    return (diff @ diff) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7524615216fba4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is the training loop. We run for a number of epochs. In each epoch, we clear the gradients, forward propagate all samples (not just a single sample) through the model, compute the loss function, execute backward propagation to get the gradient, and then make a small step (according to the learning rate) in the direction of the negative gradient. That's all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(number_of_epochs, learning_rate, theta, X_training, y_training):\n",
    "    for epoch in range(1, number_of_epochs+1):\n",
    "        # Start with a new gradient.\n",
    "        theta.grad = None\n",
    "        \n",
    "        # Compute the function values, using the current parameters.\n",
    "        y = model(X_training, theta)\n",
    "        \n",
    "        # Compute the loss, and back-propagate.\n",
    "        loss = loss_function(y, y_training)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Now make a small step in negative gradient direction.\n",
    "        with torch.no_grad():\n",
    "            theta -= learning_rate * theta.grad\n",
    "            \n",
    "        # Output something from time to time.\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch %4d Loss %f\" % (epoch, loss))\n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35a567f0c462e855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In our examples (although this is not advisable in the neural network case below), we start with $w_1=w_2=b=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-565df1ad5c12a7d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then we run the loop to get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(1000, 1e-2, theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e7b61299e58d072",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So we ended up with a MSE of approximately 0.25, which makes sense because we know we generated the data with a standard deviation of 0.5.\n",
    "\n",
    "We now modify this functions in order to record how the loss and the parameters behave during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(number_of_epochs, learning_rate, theta, X_training, y_training):\n",
    "    all_losses = []\n",
    "    all_theta = []\n",
    "    \n",
    "    for epoch in range(1, number_of_epochs+1):\n",
    "        # Start with a new gradient.\n",
    "        theta.grad = None\n",
    "        \n",
    "        # Compute the function values, using the current parameters.\n",
    "        y = model(X_training, theta)\n",
    "        \n",
    "        # Compute the loss, and back-propagate.\n",
    "        loss = loss_function(y, y_training)\n",
    "        all_losses.append(float(loss))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Now make a small step in negative gradient direction.\n",
    "        with torch.no_grad():\n",
    "            theta -= learning_rate * theta.grad\n",
    "            \n",
    "        all_theta.append(theta.detach().numpy().copy())\n",
    "            \n",
    "    return np.array(all_theta), all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1dde5a912cd76cab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then we run again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)\n",
    "thetas, losses = training_loop(1000, 1e-2, theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7da29f3a34b57b02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we can plot how the loss evolves. It converges nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbd7548fc34b3daa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also see the three parameters converging to their target values, 2, 4 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0]); plt.plot(thetas[:,1]); plt.plot(thetas[:,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d22d131f108aa33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One thing that bugs us is the learning rate. We have fixed it to 0.01. Can we achieve a faster convergence using a larger learning rate? We set it to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)\n",
    "thetas, losses = training_loop(1000, 1e-1, theta, X, y)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5623e1b5d5e56c75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0]); plt.plot(thetas[:,1]); plt.plot(thetas[:,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, this worked. It converges faster. We could stop much ahead of the 1000 iterations we used.\n",
    "\n",
    "Now greed grips us and we set the learning rate to 1, in order to be even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb3ec578fda318ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)\n",
    "thetas, losses = training_loop(1000, 1.0, theta, X, y)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e5215002287ee91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is how the parameters evolve..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0]); plt.plot(thetas[:,1]); plt.plot(thetas[:,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03bc367d3dd813cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "...here in a detailed plot of the first 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas[:50,0]); plt.plot(thetas[:50,1]); plt.plot(thetas[:50,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-60756f66235132b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Instead of computing the small step in negative gradient direction by ourselves, we can plug in an optimizer. In this case, we will use the PyTorch stochastic gradient descent optimizer, which does exactly the same thing we did. It is termed stochastic, but it just does a gradient descent for a given batch, be it a minibatch (in which case 'stochastic' makes sense) or the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([theta], lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6beb826d9b7d4b46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are some small modifications to the main loop. Instead of setting the gradient to `None` by ourselves, we now call `optimizer.zero_grad()`. Instead of our own parameter update, we can now call `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(number_of_epochs, optimizer, theta, X_training, y_training):\n",
    "    all_losses = []\n",
    "    all_theta = []\n",
    "    \n",
    "    for epoch in range(1, number_of_epochs+1):        \n",
    "        # Compute the function values, using the current parameters.\n",
    "        y = model(X_training, theta)\n",
    "        \n",
    "        # Compute the loss, and back-propagate.\n",
    "        loss = loss_function(y, y_training)\n",
    "        all_losses.append(float(loss))\n",
    "        \n",
    "        # Compute gradient and make step.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        all_theta.append(theta.detach().numpy().copy())\n",
    "            \n",
    "    return np.array(all_theta), all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_thetas, sgd_losses = training_loop(1000, optimizer, theta, X, y)\n",
    "plt.plot(sgd_losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4383e448aff42635",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Of course, since it does the same thing, we will still have the same problems if the learning rate is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([theta], lr=1.0)\n",
    "thetas, losses = training_loop(1000, optimizer, theta, X, y)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c3c5a34146d3967",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "However, now that we based everything on an optimizer, we can just plug in another, more elaborated optimizer, e.g. the `Adam` optimizer, to get a different result. This one works even if our initial learning rate is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.zeros(3, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([theta], lr=1.0)\n",
    "adam_thetas, adam_losses = training_loop(1000, optimizer, theta, X, y)\n",
    "plt.plot(adam_losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_thetas[:200,0]); plt.plot(adam_thetas[:200,1]); plt.plot(adam_thetas[:200,2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sgd_losses[:200]); plt.plot(adam_losses[:200]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some questions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-47fb0db64c665dc3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<u>Can you explain:</u>\n",
    "1. What happens if the learning rate is too small?\n",
    "2. What happens if it is too large?\n",
    "3. Can you explain the behavior of the parameter values during training which we see in all the cases above?\n",
    "\n",
    "SOME SENTENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f20591968eb03e74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Optimizing a neural network for image classification\n",
    "Now, instead of our simple single perceptron plane fit, let us use several layers for a simple image classification problem.\n",
    "\n",
    "First of all, we need a dataset. PyTorch has built-in support for a number of datasets (see the [list here](https://pytorch.org/vision/stable/datasets.html). We will follow along the lines of an example in the book by Stevens, Antiga, Viehmann: \"Deep Learning with PyTorch\", Manning Publications, and work on CIFAR-10 data. This is a collection of 60,000 very small images (32$\\times$32) from 10 classes. It is balanced, 6,000 images per class, and subdivided into a training set (50,000 images) and a validation set (10,000 images).\n",
    "\n",
    "## Loading a dataset\n",
    "\n",
    "The following lines will load the training and validation sets. The first time you execute the cell, the dataset will be automatically downloaded to the directory specified by `data_path`, which will be used as a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "data_path = './cifar10/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2ba7b468f3bc024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the dataset, classes use the class numbers 0, 1, ..., 9. The following is the corresponding class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f9c53b7ddb27d27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us look at some of the images and their labels. As noted, the images are really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2 = 8, 10\n",
    "fig = plt.figure(figsize=(n1, n2))\n",
    "for i in range(n1*n2):\n",
    "    ax = fig.add_subplot(n1, n2, 1 + i, xticks=[], yticks=[])\n",
    "    img, l = cifar10[i]\n",
    "    ax.set_title(class_names[l])\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c11aa2664172c533",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We need to do some tricks on the data. First, the dataset contains images as `PIL.Image` (python imaging library images), in a format of 32$\\times$32 pixels with 3 channels (red, green, blue), i.e., 32$\\times$32$\\times$3. However, we will need them as a tensor of dimensions channels$\\times$rows$\\times$columns.\n",
    "\n",
    "Also, generally in machine learning, we want to *normalize* the input data so that all channels have zero mean and unit standard deviation. We have computed mean and standard deviation for all channels before and just introduce the values below.\n",
    "\n",
    "Fortunately, `torchvision` allows to concatenate transforms (to check which, run `dir(transforms)`), and to specify them during loading. So we will load the `cifar10` and `cifar10_val` datasets again, this time with transforms specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                         (0.2470, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True,\n",
    "    download=False, transform=transform)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False,\n",
    "    download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-747c0e5b9eb27983",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Simplifying the problem to only two classes\n",
    "To keep our problem really simple, we will work only on two classes: airplane and bird. We will just load the data completely into memory, stored in a list of (tensor, label number) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f94cd414bdfe4ad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since each class has 5,000 samples in the training set and 1,000 samples in the validation set, we should now have 10,000 samples in the training and 2,000 samples in the validation set. Check this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cifar2), len(cifar2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-790e023d867c1d36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When training, we have the option to update the unknowns after presenting a *single* data sample, a *subset* of a few data samples, or *all* data samples. As you know from the lecture, usually a small, randomly chosen subset is used, called a *minibatch*. \n",
    "\n",
    "Fortunately, there is already a utility function in PyTorch, called `DataLoader`. In our case, we will use it to create randomly sampled (shuffled) subsets of 64 samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb8091cb026ebf84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`DataLoader` combines dataset and sampling, and provides an iterator over the (sampled, shuffled) data. To see what it does, let us 'run' the iterator for one epoch, printing the shape of the presented data. As we see, 64 images with 3 channels and 32$\\times$32 pixels, i.e. a 64$\\times$3$\\times$32$\\times$32 tensor, is the training data. It is accompanied by 64 labels. This together is one minibatch. Since 10,000 = 64 $\\times$ 156 + 16, the last minibatch will contain only 16 samples (scroll down to spot this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(d[0].shape, d[1].shape) for d in train_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16686862a3ad154f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setting up the model and loss function\n",
    "In the plane fitting above, our model was the affine function, and the loss function was the mean squared error.\n",
    "\n",
    "Now, we will have more complicated models, which can fortunately be specified in PyTorch in a quite comfortable way. We will use fully connected layers as follows:\n",
    "- The first layer will be a linear (affine) layer. Since the images are 3$\\times$32$\\times$32, it will have a 3,072 input. It will then reduce this to a 128 features. I.e., 128 is the number of features in the hidden layer.\n",
    "- As activation function, we will use the hyperbolic tangent, `Tanh`.\n",
    "- Then, another layer will reduce the 128 features to only two features. This is how we represent the outcome of our classification: as two features, one for airplane, and one for bird.\n",
    "- As we have seen in the lecture, we can convert this output to 'probabilities' using the `softmax` function. Then, we compute the negative log likelihood on that. This is done in our implementation by using the `LogSoftmax` layer, together with the `NLLLoss`.\n",
    "\n",
    "All those layers are connected in succession - this can be comfortably achieved using the `Sequential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b608058be8379636",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setting up the optimizer and running the training\n",
    "\n",
    "Next, we need an optimizer. We will just take the plain stochastic gradient descent. Remember from above, we had to hand over the parameters to be optimized as tensors (with `requires_grad=True`). However, the parameters are now all hidden inside the model, none of them was created by ourselves directly. Fortunately, we can just call `model.parameters()` to get all parameters inside our model. In our case, this is a 3,072$\\times$128 weight matrix for the connections between the first layer and the hidden layer, a 128-element bias vector of the hidden layer, another 128$\\times$2 weight matrix for the connections between the hidden layer and the output layer, and a 2-element bias vector for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df4cc0d892597ad8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next comes the training loop. It is very similar to the loop we used above in the plane fitting example. Since our image batches are batch$\\times$3$\\times$32$\\times$32, but our network expects batch$\\times$3072, we alter the shape using `view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "all_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        all_losses.append(float(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: %2d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ae300807f5d0999",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Analysing the behaviour\n",
    "In the record, we aggregate the losses for each minibatch (157), over all 100 epochs, for a total of 15700 loss values. When we plot them, we see that generally, the loss is reduced during training. Of course, since the losses are per minibatch, we also see quite some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b976a8b4e3fa130c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to count how many correct classifications we have, we use the following code. It computes the proportion of correct classifications, for the training data set, reported as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2944bd9a1fb60012",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can do the same for the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ab9d6d34a3282f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Out of curiosity, we can ask the model for the total number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b4d3ce8a3ddf88e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Optimizing a convolutional neural network\n",
    "We will now use a *convolutional neural network*, instead of the fully connected model, to solve the same task.\n",
    "\n",
    "First of all, we define the model. It is also in layers, but we use a slightly different way of describing it. Instead of `nn.Sequential()`, we now define a new class, `Net`, which instantiates all layers in the constructor. It has another member function, `forward()`, which computes a forward propagation of a sample. As we know, this forward propagation will also establish the computation graph which is then later used for backpropagation. In detail, our network has:\n",
    "- an input of three channel (C), 32$\\times$32 tensors, i.e. 3C$\\times$32$\\times$32\n",
    "- a 3$\\times$3 convolution from 3 channels to 16 channels, followed by a `Tanh` activation and a maxpool operation with window size (and stride) 2, resulting in 16C$\\times$16$\\times$16\n",
    "- another 3$\\times$3 convolution from 16 channels to 8 channels, followed by `Tanh` and maxpool, resulting in 8C$\\times$8$\\times$8,\n",
    "- a reshape (using `view`) from 8C$\\times$8$\\times$8 to a 512-dimensional (D) feature vector,\n",
    "- a linear layer from 512D to 32D, followed by `Tanh`, and\n",
    "- another linear layer, reducing from 32D to the required 2D output.\n",
    "\n",
    "The combination of a `LogSoftmax` layer and the `NLLLoss()` loss function (which we used above) is replaced by `CrossEntropyLoss`, which computes the same loss. (Although the loss is the same, the last layer of our network now does not explicitly represent probabilities or log probabilities. So to get them for test data, we would have to add a `Softmax` layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d48e3e0144d2b86e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let us run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "model = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "n_epochs = 100\n",
    "all_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        all_losses.append(float(loss))\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: %2d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b83afe243a77e71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As above, we can have a look at the losses in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a1102a0e90a1117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And we can look at the training accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41068f9618c24d63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "...as well as the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b333c94f55cd30a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Fortunately, when we assembled all the layers in our `Net` class above, some secret mechanism has kept track of all the parameters. So as before, we can just ask for the parameters using `model.parameters()`, and thus count the total number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d58c6892f2beaa2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Some questions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a448fa1fc605c285",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<u>You have run two networks for image classification. What is the major difference between them? Which one makes more sense, and why?</u>\n",
    "\n",
    "SOME SENTENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-04982a5bcafc8874",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<u>Above, we plotted the evolution of the loss function for both networks. Please comment on the common aspects and their differences.</u>\n",
    "\n",
    "SOME SENTENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bcecdc8c638b7480",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<u>We have also output the accuracy for the training and the validation datasets, and the number of parameters. What kind of conclusions do you draw from these figures?</u>\n",
    "\n",
    "SOME SENTENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-da2b461ba559a43a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<u>What would you suggest to improve the performance of our network, considering what you learned in the lecture? (Remember that it would even get more complicated when we switch to 10 classes.) </u>\n",
    "\n",
    "SOME SENTENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "copyright": {
   "affiliation": "IKG, Leibniz University Hannover",
   "name": "Claus Brenner"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
